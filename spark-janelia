#!/usr/local/python-2.7.6/bin/python2.7

import os
import glob
import sys
import argparse
import subprocess
import xml.etree.ElementTree as ET
import time
import multiprocessing


def findMasterByJobId(jobId):
    status = subprocess.check_output(['qstat', '-xml'])
    master = ''
    qtree = ET.fromstring(status)[0]
    rjobs = qtree.findall(".//job_list[@state='running']")
    for job in rjobs:
        if not isSparkJob( job ):
            continue
        currMaster = job.find('queue_name').text.split('@')[1]
        currJobId  = job.find('JB_job_number').text
        if jobId == '' or currJobId == jobId:
            master = currMaster
            break
    return master


def isSparkJob(job):
    return job.find('jclass_name').text == jobname + '.default'


def launch(sleepTime='86400'):
    if not os.path.exists(os.path.expanduser('~/sparklogs')):
        os.mkdir(os.path.expanduser('~/sparklogs'))

    startupCommand = '/groups/freeman/home/freemanj11/startspark.sh'
    output = subprocess.check_output(['qsub', '-jc', jobname, '-pe', jobname, str(args.nnodes), '-q', 'hadoop2', '-j', 'y', '-o', os.path.expanduser('~/sparklogs/'), startupCommand, version])
    print('\n')
    print('Spark job submitted with ' + str(args.nnodes) + ' nodes')
    print('\n')
    # Your job <jobId> ("Sleeper") has been submitted
    jobId = output.split(' ')[2]
    return jobId

def login():
    status = subprocess.check_output(["qstat", "-xml"])
    qtree = ET.fromstring(status)[0]
    jtree = ET.fromstring(status)[1]
    jobs = qtree.findall('job_list')
    if len(jobs) == 0:
        print('\n')
        print >> sys.stderr, "No running jobs found, keep waiting for jobs to launch"
        print('\n')
    else:
        address = None
        for job in jobs:
            jobclass = job.find('jclass_name').text
            state = job.find('state').text
            jobid = int(job.find('JB_job_number').text)
            if (jobclass == jobname + ".default") and (state == 'r'):
                if args.jobid:
                    if jobid == args.jobid:
                        address = job.find('queue_name').text.split('@')[1]
                else:
                    address = job.find('queue_name').text.split('@')[1]
        if address:
            filename = os.path.expanduser('~/spark-master')
            if os.path.exists(filename):
                os.remove(filename)
            os.system("echo 'spark://'" + address + ":7077 >> " + os.path.expanduser("~") + "/spark-master")
            #subprocess.call(['qlogin', '-pe', 'batch', '16', '-l', 'sandy'])
            subprocess.check_call(['ssh', address])
        else:
            print('\n')
            print >> sys.stderr, "No Spark job found, check status with qstat, or try a different jobid?"
            print('\n')


def destroy(jobId ):
    status = subprocess.check_output(["qstat", "-xml"])
    qtree = ET.fromstring(status)[0]
    jtree = ET.fromstring(status)[1]
    jobs = qtree.findall('job_list')
    if len(jobs) == 0:
        print('\n')
        print >> sys.stderr, "No running jobs found"
        print('\n')
    else:
        deleted = 0
        for job in jobs:
            jobclass = job.find('jclass_name').text
            state = job.find('state').text
            jobid = job.find('JB_job_number').text
            if (jobclass == jobname + ".default") and (state == 'r'):
                if jobId and jobid == jobId:
                    output = subprocess.check_output(['qdel', jobid])
                    deleted += 1
                else:
                    output = subprocess.check_output(['qdel', jobid])
                    deleted += 1
        if deleted == 0:
            print('\n')
            print >> sys.stderr, "No Spark jobs deleted, try a different jobid?"
            print('\n')
        else:
            print('\n')
            print('Spark jobs successfully deleted')
            print('\n')


def start():
    f = open(os.path.expanduser("~") + '/spark-master', 'r')
    master = f.readline().replace('\n','')

    if args.ipython is True:
       os.environ['IPYTHON_OPTS'] = "notebook --profile=nbserver"
       notebook = master[8:][:-5]
       print('\n')
       print('View your notebooks at http://' + notebook + ':9999')
       print('\n')

    os.environ['SPARK_HOME'] = version

    if os.getenv('PATH') is None:
        os.environ['PATH'] = ""

    if os.getenv('IPYTHON') is None:
       os.environ['IPYTHON'] = "1"

    os.environ['PATH'] = os.environ['PATH'] + ":" + "/usr/local/python-2.7.6/bin"
    os.environ['MASTER'] = master
    os.system(version + '/bin/pyspark')


def startScala():
    os.environ['SPARK_HOME'] = version

    if os.getenv('PATH') is None:
        os.environ['PATH'] = ""

    os.environ['PATH'] = os.environ['PATH'] + ":" + "/usr/local/python-2.7.6/bin"
    f = open(os.path.expanduser("~") + '/spark-master', 'r')
    os.environ['MASTER'] = f.readline().replace('\n','')
    os.system(version + '/bin/spark-shell')


def submit(master = ''):
    os.environ['SPARK_HOME'] = version

    if os.getenv('PATH') is None:
        os.environ['PATH'] = ""

    os.environ['PATH'] = os.environ['PATH'] + ":" + "/usr/local/python-2.7.6/bin"
    if master == '':
        with open(os.path.expanduser("~") + '/spark-master', 'r') as f:
            master = f.readline().replace('\n','')
    os.environ['MASTER'] = master
    os.system(version + '/bin/spark-submit --master ' + master + ' ' + args.submitargs)


def launchAndWait():
        jobId  = launch(str(args.sleep_time))
        master = ''     
        while( master == '' ):
            master = findMasterByJobId(jobId)
            time.sleep(1) # wait 1 second to avoid spamming the cluster
            sys.stdout.write('.')
            sys.stdout.flush()
        return master


def submitAndDestroy( master, jobId ):
    submit(master)
    destroy(jobId)

if __name__ == "__main__":

    parser = argparse.ArgumentParser(description="launch and manage spark cluster jobs")
                        
    choices = ('launch', 'login', 'destroy', 'start', 'start-scala', 'submit', 'lsd', 'launch-in')
                        
    parser.add_argument("task", choices=choices)
    parser.add_argument("-n", "--nnodes", type=int, default=2, required=False)
    parser.add_argument("-i", "--ipython", action="store_true")
    parser.add_argument("-v", "--version", choices=("stable", "rc", "test"), default="stable", required=False)
    parser.add_argument("-j", "--jobid", type=int, default=None, required=False)
    parser.add_argument("-t", "--sleep_time", type=int, default=86400, required=False)
    parser.add_argument("-s", "--submitargs", type=str, default='', required=False)
                        
    args = parser.parse_args()
                        
    SPARKVERSIONS = {   
        'stable': '/usr/local/spark-current/',
        'rc': '/usr/local/spark-rc/',
        'test': '/usr/local/spark-test'
    }                   
                        
    SPARKJOBS = {
        'stable': 'spark',   
        'rc': 'spark-rc',  
        'test': 'spark-test'   
    }    

    version = SPARKVERSIONS[args.version]
    jobname = SPARKJOBS[args.version]
                        
    if args.nnodes == 1:
        raise Exception('Cannot start a Spark cluster with only 1 node, please request 2 or more.')
                        
    if args.task == 'launch':
        launch(str(args.sleep_time))
                        
    elif args.task == 'login':
        login()         
                        
    elif args.task == 'destroy':
        destroy(args.jobid or '')
                        
    elif args.task == 'start':
        start()         
                        
    elif args.task == 'start-scala':
        startScala()   
                        
    elif args.task == 'submit':
        submit()        
                        
    elif args.task == 'lsd':
        master = launchAndWait()
        master = 'spark://%s:7077' % master
        print('\n')     
        print('%-20s%s\n%-20s%s' % ( 'job id:', jobId, 'spark master:', master ) )
        print('\n')     
        p = multiprocessing.Process(target=submitAndDestroy, args=(master, jobId))
        p.start()       

    elif args.task == 'launch-in':
        master = launchAndWait()
        print '\n\nspark master: {}\n'.format(master)
        login()


