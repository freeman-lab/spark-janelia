#!/usr/bin/env python

import argparse
import collections
import datetime
import os
import string
import subprocess
import sys
import time


def ensure_python_3_6():
    if sys.version_info < (3, 6):
        raise Exception("requires python 3.6 or later")
    # noinspection PyUnusedLocal
    test_f_string = f'if this line fails to be parsed with a SyntaxError, you need to run with python 3.6 or later'


def parse_args():
    parser = argparse.ArgumentParser(description="launch batch spark cluster job")

    # -----------------------------------------------------------------
    # legacy args used by https://github.com/saalfeldlab/flintstone/blob/master/flintstone-lsd.sh ...

    parser.add_argument("task", choices=['generate-run', 'generate-and-launch-run'],
                        help='use generate-run if you want to review scripts before launching')
    parser.add_argument("-n", "--nnodes", type=int, default=2, required=False,
                        help='maximum number of worker nodes to schedule')
    parser.add_argument("-v", "--version", type=str, default='3.0.1', required=False,
                        help='shortcut for specifying SPARK_HOMEs in /misc/local/spark-*')
    parser.add_argument("-t", "--hard_runtime", type=str, default='8:00', required=False,
                        help='maximum number of minutes to allow jobs to run, '
                             'define as minutes or as hours:minutes')
    parser.add_argument("-s", "--submitargs", type=str, required=True,
                        help='arguments to pass to spark-submit when launching driver, '
                             'e.g. --class ${DRIVER_CLASS} ${DRIVER_JAR} ${DRIVER_ARGS}')
    parser.add_argument("-d", "--driverslots", type=int, default=32, required=False,
                        help='number of CPU slots to allocate for the driver')
    parser.add_argument("-P", "--lsf_project", type=str, default=None, required=False,
                        help='LSF project (or sge account) for cluster job billing')
    parser.add_argument("--minworkers", type=int, default=1, required=False,
                        help='minimum number of workers required to be running before starting driver')

    # -----------------------------------------------------------------
    # new args ...

    home_dir = os.getenv('HOME')
    user_name = os.getenv('USER')

    parser.add_argument("--common_job_args", type=str, default='', required=False,
                        help='arguments passed to all job submissions (e.g. -q test)')
    parser.add_argument("--consolidate_logs", action="store_true",
                        help='if specified, overrides --run_worker_dir and writes worker logs to run directory')
    parser.add_argument("--gb_per_slot", type=int, default=15, required=False,
                        help='Janelia cluster resource info listed here: '
                             'http://wiki.int.janelia.org/wiki/display/ScientificComputing/Janelia+Compute+Cluster')
    parser.add_argument("--hpc_type", choices=['lsf', 'sge'], default='lsf', required=False,
                        help='high performance cluster type')
    parser.add_argument("--java_home", type=str, default=os.getenv('JAVA_HOME'), required=False,
                        help='explicit JAVA_HOME directory')
    parser.add_argument("--run_local_dirs", type=str, default=None, required=False,
                        help='override default storage directories used on worker nodes for shuffle and RDD data')
    parser.add_argument("--run_parent_dir", type=str, default=f'{home_dir}/.spark', required=False,
                        help='override default parent directory for run scripts and logs')
    parser.add_argument("--run_worker_dir", type=str, default=None, required=False,
                        help='override default directory on worker nodes containing worker jar and log files')
    parser.add_argument("--spark_home", type=str, default=os.getenv('SPARK_HOME'), required=False,
                        help='explicit SPARK_HOME directory, overrides --version')
    parser.add_argument("--worker_slots", type=int, default=32, required=False,
                        help='slots per worker node')

    args = parser.parse_args()

    # -----------------------------------------------------------------
    # setup args with more complex validation or derived defaults

    java_home = args.java_home if args.java_home else '/misc/local/jdk1.8.0_102'
    if not os.path.isfile(f'{java_home}/bin/java'):
        raise ValueError(f'invalid java home: {java_home}')
    args.java_home = java_home

    spark_home = args.spark_home if args.spark_home else f'/misc/local/spark-{args.version}'
    if not os.path.isfile(f'{spark_home}/bin/spark-submit'):
        raise ValueError(f'invalid spark home: {spark_home}')
    args.spark_home = spark_home

    worker_scratch_dir = f'/scratch/{user_name}/spark'
    if args.hpc_type == 'lsf':
        lsf_job_id_vars = '${LSB_JOBID}-${LSB_JOBINDEX}'
        worker_scratch_dir = f'/scratch/{user_name}/spark-{lsf_job_id_vars}'

        if args.lsf_project:
            args.common_job_args = f'-P {args.lsf_project} {args.common_job_args}'.strip()

    elif args.hpc_type == 'sge':
        # sge_job_id_vars = '${JOB_ID}-${SGE_TASK_ID}'
        worker_scratch_dir = '${TMPDIR}'

        if args.lsf_project:
            args.common_job_args = f'-A {args.lsf_project} {args.common_job_args}'.strip()

    if not args.run_local_dirs:
        # NOTE: when Spark jobs were confined to a full node, Janelia used to define local dirs as:
        # /data1/sparklocaldir,/data2/sparklocaldir,...,/data6/sparklocaldir
        args.run_local_dirs = worker_scratch_dir

    if not args.run_worker_dir:
        args.run_worker_dir = worker_scratch_dir

    return args


class SparkTemplate(string.Template):
    delimiter = '@'
    idpattern = r'[a-z][_a-z0-9]*'


def render_template(relative_template_path,
                    to_path,
                    **kwargs):

    spark_janelia_path = os.path.realpath(__file__)
    spark_janelia_dir = os.path.dirname(spark_janelia_path)
    template_path = f'{spark_janelia_dir}/templates/{relative_template_path}'

    with open(template_path, 'r') as template_file:
        template_contents = template_file.read()

    template = SparkTemplate(template_contents)
    populated_template = template.substitute(kwargs)

    with open(to_path, "w") as file:
        print(populated_template, file=file)

    return to_path


def write_common_config_files(args, run_info):

    driver_memory = (args.driverslots * args.gb_per_slot) - 1  # leave 1GB for Spark infrastructure on driver
    worker_memory = args.worker_slots * args.gb_per_slot

    render_template('conf/log4j.properties',
                    f'{run_info.config_dir}/log4j.properties')

    render_template('conf/spark-defaults.conf',
                    f'{run_info.config_dir}/spark-defaults.conf',
                    driver_memory=driver_memory)

    # For supported SPARK_WORKER_OPTS, see https://spark.apache.org/docs/latest/spark-standalone.html .
    worker_opts = ''

    # If worker logs are not being consolidated into the central log directory
    # where the driver and other logs are written (presumably for debug),
    # enable automatic cleanup and set a short interval and time-to-live
    # so that workers cleanup before the shutdown job kills them.
    if not args.consolidate_logs:
        worker_opts='-Dspark.worker.cleanup.enabled=true ' \
                    '-Dspark.worker.cleanup.interval=30 ' \
                    '-Dspark.worker.cleanup.appDataTtl=1'

    render_template('conf/spark-env.sh',
                    f'{run_info.config_dir}/spark-env.sh',
                    java_home=args.java_home,
                    run_local_dirs=run_info.local_dirs,
                    run_logs_dir=run_info.logs_dir,
                    run_worker_dir=run_info.worker_dir,
                    spark_home=args.spark_home,
                    worker_memory=worker_memory,
                    worker_opts=worker_opts)


def write_common_scripts(args, run_info):

    script_path = render_template('scripts/01-launch-master.sh',
                                  f'{run_info.scripts_dir}/01-launch-master.sh',
                                  run_config_dir=run_info.config_dir,
                                  spark_home=args.spark_home)
    os.chmod(script_path, 0o755)

    script_path = render_template('scripts/02-save-master-url.sh',
                                  f'{run_info.scripts_dir}/02-save-master-url.sh',
                                  master_url_path=run_info.master_url_path)
    os.chmod(script_path, 0o755)

    script_path = render_template('scripts/03-launch-worker.sh',
                                  f'{run_info.scripts_dir}/03-launch-worker.sh',
                                  master_url_path=run_info.master_url_path,
                                  run_config_dir=run_info.config_dir,
                                  spark_home=args.spark_home)
    os.chmod(script_path, 0o755)

    script_path = render_template('scripts/04-launch-driver.sh',
                                  f'{run_info.scripts_dir}/04-launch-driver.sh',
                                  master_url_path=run_info.master_url_path,
                                  run_config_dir=run_info.config_dir,
                                  spark_home=args.spark_home,
                                  submit_args=args.submitargs)
    os.chmod(script_path, 0o755)


def write_lsf_scripts(args, run_info):

    script_path = render_template('scripts/00-queue-lsf-jobs.sh',
                                  f'{run_info.launch_script}',
                                  common_job_args=args.common_job_args,
                                  driver_slots=args.driverslots,
                                  job_name_prefix=run_info.job_name_prefix,
                                  job_runtime_limit=args.hard_runtime,
                                  max_worker_nodes=args.nnodes,
                                  min_worker_nodes=args.minworkers,
                                  run_logs_dir=run_info.logs_dir,
                                  run_scripts_dir=run_info.scripts_dir,
                                  worker_slots=args.worker_slots)
    os.chmod(script_path, 0o755)

    script_path = render_template('scripts/05-shutdown-lsf-jobs.sh',
                                  f'{run_info.scripts_dir}/05-shutdown-lsf-jobs.sh',
                                  consolidate_logs=args.consolidate_logs,
                                  job_name_prefix=run_info.job_name_prefix,
                                  run_logs_dir=run_info.logs_dir)
    os.chmod(script_path, 0o755)


def write_sge_scripts(args, run_info):

    if ':' in args.hard_runtime:
        t = datetime.datetime.strptime(args.hard_runtime, "%H:%M")
        job_max_run_seconds = int(datetime.timedelta(hours=t.hour, minutes=t.minute).total_seconds())
    else:
        job_max_run_seconds = 60 * int(args.hard_runtime)

    script_path = render_template('scripts/10-queue-sge-jobs.sh',
                                  f'{run_info.launch_script}',
                                  common_job_args=args.common_job_args,
                                  driver_slots=args.driverslots,
                                  job_max_run_seconds=job_max_run_seconds,
                                  job_name_prefix=run_info.job_name_prefix,
                                  max_worker_nodes=args.nnodes,
                                  min_worker_nodes=args.minworkers,
                                  run_logs_dir=run_info.logs_dir,
                                  run_scripts_dir=run_info.scripts_dir,
                                  worker_slots=args.worker_slots)
    os.chmod(script_path, 0o755)

    script_path = render_template('scripts/11-verify-sge-workers-ready.sh',
                                  f'{run_info.scripts_dir}/11-verify-sge-workers-ready.sh')
    os.chmod(script_path, 0o755)

    script_path = render_template('scripts/12-shutdown-sge-jobs.sh',
                                  f'{run_info.scripts_dir}/12-shutdown-sge-jobs.sh',
                                  consolidate_logs=args.consolidate_logs,
                                  job_name_prefix=run_info.job_name_prefix,
                                  run_logs_dir=run_info.logs_dir)
    os.chmod(script_path, 0o755)


RunInfo = collections.namedtuple(
    'RunInfo',
    'dir, config_dir, logs_dir, scripts_dir, launch_script, worker_dir, master_url_path, local_dirs, job_name_prefix')


def create_run(args):

    timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')
    run_dir = f'{args.run_parent_dir}/{timestamp}'

    try:
        os.makedirs(run_dir)
    except FileExistsError:
        # retry once to work around rare case of concurrent run by same user
        time.sleep(1)
        timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')
        run_dir = f'{args.run_parent_dir}/{timestamp}'

    run_config_dir = f'{run_dir}/conf'
    run_logs_dir = f'{run_dir}/logs'
    run_scripts_dir = f'{run_dir}/scripts'
    launch_script = f'{run_scripts_dir}/00-queue-{args.hpc_type}-jobs.sh'
    master_url_path = f'{run_dir}/master-url.txt'

    run_worker_dir = args.run_worker_dir
    if args.consolidate_logs:
        if args.hpc_type == 'lsf':
            job_index = '${LSB_JOBINDEX}'
            run_worker_dir = f'{run_dir}/logs/worker-{job_index}-dir'

        elif args.hpc_type == 'sge':
            task_id = '${SGE_TASK_ID}'
            run_worker_dir = f'{run_dir}/logs/worker-{task_id}-dir'

    user_name = os.getenv('USER')
    job_name_prefix = f'spark_{user_name}_{timestamp}'

    os.makedirs(run_config_dir)
    os.makedirs(run_logs_dir)
    os.makedirs(run_scripts_dir)

    print(f'\nCreated:\n  {run_config_dir}\n  {run_logs_dir}\n  {run_scripts_dir}\n')

    return RunInfo(run_dir, run_config_dir, run_logs_dir, run_scripts_dir, launch_script,
                   run_worker_dir, master_url_path, args.run_local_dirs, job_name_prefix)


def main():

    ensure_python_3_6()

    args = parse_args()
    run_info = create_run(args)

    if args.hpc_type == 'lsf':
        write_lsf_scripts(args, run_info)
    elif args.hpc_type == 'sge':
        write_sge_scripts(args, run_info)

    write_common_config_files(args, run_info)
    write_common_scripts(args, run_info)

    if args.task == 'generate-and-launch-run':
        print(f'Running:\n  {run_info.launch_script}\n')
        subprocess.run(run_info.launch_script)
    else:
        print(f'To launch jobs, run:\n  {run_info.launch_script}\n')

    print('\nTo get web user interface URL after master has started, run:')
    print(f'  grep "Bound MasterWebUI to" {run_info.logs_dir}/01-master.log\n')


if __name__ == "__main__":
    main()
