#!/misc/local/python-2.7.11/bin/python2.7

import os
import glob
import sys
import argparse
import subprocess
import xml.etree.ElementTree as ET
import time
import multiprocessing


def findMasterByJobId(jobId):
    status = subprocess.check_output(['qstat', '-xml'])
    master = ''
    qtree = ET.fromstring(status)[0]
    rjobs = qtree.findall(".//job_list[@state='running']")
    for job in rjobs:
        if not isSparkJob( job ):
            continue
        currMaster = job.find('queue_name').text.split('@')[1]
        currJobId  = job.find('JB_job_number').text
        if jobId == '' or currJobId == jobId:
            master = currMaster
            break
    return master


def isSparkJob(job):
    return job.find('jclass_name').text == jobname + '.default'


def launch(sleepTime='86400'):
    if not os.path.exists(os.path.expanduser('~/sparklogs')):
        os.mkdir(os.path.expanduser('~/sparklogs'))

    startupCommand = '/misc/local/spark-versions/bin/' + launchscript
    output = subprocess.check_output(['qsub', '-jc', jobname, '-pe', jobname, str(args.nnodes), '-o', os.path.expanduser('~/sparklogs/'), startupCommand])
    print('\n')
    print('Spark job submitted with ' + str(args.nnodes) + ' nodes')
    print('\n')
    jobId = output.split(' ')[2]
    return jobId

def login():
    status = subprocess.check_output(["qstat", "-xml"])
    qtree = ET.fromstring(status)[0]
    jtree = ET.fromstring(status)[1]
    jobs = qtree.findall('job_list')
    if len(jobs) == 0:
        print('\n')
        print >> sys.stderr, "No running jobs found, keep waiting for jobs to launch"
        print('\n')
    else:
        address = None
        for job in jobs:
            procname = job.find('JB_name').text
            jclass = job.find('jclass_name').text
            state = job.find('state').text
            jobid = int(job.find('JB_job_number').text)
            if jclass and (jclass[:5] == 'spark') and (state == 'r'):
                if args.jobid:
                    if jobid == args.jobid:
                        address = job.find('queue_name').text.split('@')[1]
                else:
                    address = job.find('queue_name').text.split('@')[1]
        if address:
            filename = os.path.expanduser('~/spark-master')
            if os.path.exists(filename):
                os.remove(filename)
            os.system("echo 'spark://'" + address + ":7077 >> " + os.path.expanduser("~") + "/spark-master")
            subprocess.call(['qlogin', '-pe', 'batch', '16', '-l', 'interactive=true'])
        else:
            print('\n')
            print >> sys.stderr, "No Spark job found, check status with qstat, or try a different jobid?"
            print('\n')


def destroy(jobId ):
    status = subprocess.check_output(["qstat", "-xml"])
    qtree = ET.fromstring(status)[0]
    jtree = ET.fromstring(status)[1]
    jobs = qtree.findall('job_list')
    if len(jobs) == 0:
        print('\n')
        print >> sys.stderr, "No running jobs found"
        print('\n')
    else:
        deleted = 0
        for job in jobs:
            procname = job.find('JB_name').text
            jclass = job.find('jclass_name').text
            state = job.find('state').text
            jobid = job.find('JB_job_number').text
            if (jclass[:5] == 'spark') and (state == 'r'):
                if jobId and jobid == jobId:
                    output = subprocess.check_output(['qdel', jobid])
                    deleted += 1
                else:
                    output = subprocess.check_output(['qdel', jobid])
                    deleted += 1
        if deleted == 0:
            print('\n')
            print >> sys.stderr, "No Spark jobs deleted, try a different jobid?"
            print('\n')
        else:
            print('\n')
            print('Spark jobs successfully deleted')
            print('\n')


def start():
    f = open(os.path.expanduser("~") + '/spark-master', 'r')
    master = f.readline().replace('\n','')

    if args.notebook is True or args.ipython is True:
       os.environ['PYSPARK_DRIVER_PYTHON'] = 'ipython'
    
    if args.notebook is True:
       os.environ['PYSPARK_DRIVER_PYTHON'] = 'jupyter'
       os.environ['PYSPARK_DRIVER_PYTHON_OPTS'] = 'notebook --ip "*" --port 9999 --no-browser'
       address = master[8:][:-5]
       print('\n')
       print('View your notebooks at http://' + os.environ['HOSTNAME'] + ':9999')
       print('View the status of your cluster at http://' + address + ':8080')
       print('\n')


    if os.getenv('PATH') is None:
        os.environ['PATH'] = ""

    os.system(version + '/bin/pyspark --master='+master)


def startScala():
    os.environ['SPARK_HOME'] = version

    if os.getenv('PATH') is None:
        os.environ['PATH'] = ""

    os.environ['PATH'] = os.environ['PATH'] + ":" + "/misc/local/python-2.7.11/bin"
    f = open(os.path.expanduser("~") + '/spark-master', 'r')
    os.environ['MASTER'] = f.readline().replace('\n','')
    os.system(version + '/bin/spark-shell')


def submit(master = ''):
    os.environ['SPARK_HOME'] = version

    if os.getenv('PATH') is None:
        os.environ['PATH'] = ""

    os.environ['PATH'] = os.environ['PATH'] + ":" + "/misc/local/python-2.7.11/bin"
    if master == '':
        with open(os.path.expanduser("~") + '/spark-master', 'r') as f:
            master = f.readline().replace('\n','')
    os.environ['MASTER'] = master

    # ssh into master and then run spark-submit
    currentPath = os.getcwd();
    command = 'ssh ' + master[8:14] +  ' "cd ' + currentPath + '; ' + version + '/bin/spark-submit --master ' + master + ' ' + args.submitargs + '"'
    os.system(command)


def launchAndWait():
        jobId  = launch(str(args.sleep_time))
        master = ''     
        while( master == '' ):
            master = findMasterByJobId(jobId)
            time.sleep(1) # wait 1 second to avoid spamming the cluster
            sys.stdout.write('.')
            sys.stdout.flush()
        return master, jobId

def update():
    currentdir = os.getcwd()
    scriptdir = os.path.dirname(os.path.realpath(__file__))
    os.chdir(scriptdir)
    os.system('git pull origin master')
    os.chdir(currentdir)

def submitAndDestroy( master, jobId ):
    submit(master)
    destroy(jobId)

if __name__ == "__main__":

    parser = argparse.ArgumentParser(description="launch and manage spark cluster jobs")
                        
    choices = ('launch', 'login', 'destroy', 'start', 'start-scala', 'submit', 'lsd', 'launch-in', 'update')
                        
    parser.add_argument("task", choices=choices)
    parser.add_argument("-n", "--nnodes", type=int, default=2, required=False)
    parser.add_argument("-i", "--ipython", action="store_true")
    parser.add_argument("-b", "--notebook", action="store_true")
    parser.add_argument("-v", "--version", choices=("stable", "rc", "test", "2"), default="stable", required=False)
    parser.add_argument("-j", "--jobid", type=int, default=None, required=False)
    parser.add_argument("-t", "--sleep_time", type=int, default=86400, required=False)
    parser.add_argument("-s", "--submitargs", type=str, default='', required=False)
                        
    args = parser.parse_args()
                        
    SPARKVERSIONS = {   
        'stable': '/misc/local/spark-current/',
        'rc': '/misc/local/spark-rc/',
        'test': '/misc/local/spark-test',
        '2': '/misc/local/spark-2'
    }                   
                        
    SPARKJOBS = {
        'stable': 'spark',   
        'rc': 'spark-rc',  
        'test': 'spark-test',   
        '2': 'spark-2'
    }    

    SPARKLAUNCHSCRIPTS = {
        'stable': 'start-cluster.sh',
        'rc': 'start-rc-cluster.sh',
        '2': 'start-2-cluster.sh'
    }

    version = SPARKVERSIONS[args.version]
    jobname = SPARKJOBS[args.version]
    launchscript = SPARKLAUNCHSCRIPTS[args.version]
                        
    if args.nnodes == 1:
        raise Exception('Cannot start a Spark cluster with only 1 node, please request 2 or more.')
                        
    if args.task == 'launch':
        launch(str(args.sleep_time))
                        
    elif args.task == 'login':
        login()         
                        
    elif args.task == 'destroy':
        destroy(args.jobid or '')
                        
    elif args.task == 'start':
        start()         
                        
    elif args.task == 'start-scala':
        startScala()   
                        
    elif args.task == 'submit':
        submit()        
                        
    elif args.task == 'lsd':
        master, jobId = launchAndWait()
        master = 'spark://%s:7077' % master
        print('\n')     
        print('%-20s%s\n%-20s%s' % ( 'job id:', jobId, 'spark master:', master ) )
        print('\n')     
        p = multiprocessing.Process(target=submitAndDestroy, args=(master, jobId))
        p.start()       

    elif args.task == 'launch-in':
        master, jobId = launchAndWait()
        print '\n\nspark master: {}\n'.format(master)
        login()

    elif args.task == 'update':
        update()


