#!/usr/local/python-2.7.6/bin/python2.7

import os
import glob
import sys
import argparse
import subprocess
import xml.etree.ElementTree as ET
import time
import multiprocessing


def findMasterByJobId(jobId):
    status = subprocess.check_output(['qstat', '-xml'])
    master = ''
    qtree = ET.fromstring(status)[0]
    rjobs = qtree.findall(".//job_list[@state='running']")
    for job in rjobs:
        if not isSparkJob( job ):
            continue
        currMaster = job.find('queue_name').text.split('@')[1]
        currJobId  = job.find('JB_job_number').text
        if jobId == '' or currJobId == jobId:
            master = currMaster
            break
    return master


def isSparkJob(job):
    return job.find('jclass_name').text == version_short + '.default'


def launch(sleepTime='86400'):
    if not os.path.exists(os.path.expanduser('~/sparklogs')):
        os.mkdir(os.path.expanduser('~/sparklogs'))

    output = subprocess.check_output(['qsub', '-jc', version_short, '-pe', version_short, str(args.nnodes), '-q', 'hadoop2', '-j', 'y', '-o', os.path.expanduser('~/sparklogs/'), '/sge/8.1.4/examples/jobs/sleeper.sh', sleepTime])
    print('\n')
    print('Spark job submitted with ' + str(args.nnodes) + ' nodes')
    print('\n')
    # Your job <jobId> ("Sleeper") has been submitted
    jobId = output.split(' ')[2]
    return jobId


def login(jobId=''):
    status = subprocess.check_output(["qstat", "-xml"])
    qtree = ET.fromstring(status)[0]
    jtree = ET.fromstring(status)[1]
    jobs = qtree.findall('job_list')
    if len(jobs) == 0:
        print('\n')
        print >> sys.stderr, "No running jobs found, keep waiting for jobs to launch"
        print('\n')
    else:
        address = None
        for job in jobs:
            jobclass = job.find('jclass_name').text
            state = job.find('state').text
            jobid = int(job.find('JB_job_number').text)
            if (jobclass == version_short + ".default") and (state == 'r'):
                if jobId:
                    if jobid == jobId:
                        address = job.find('queue_name').text.split('@')[1]
                else:
                    address = job.find('queue_name').text.split('@')[1]
        if address:
            filename = os.path.expanduser('~/spark-master')
            if os.path.exists(filename):
                os.remove(filename)
            os.system("echo 'spark://'" + address + ":7077 >> " + os.path.expanduser("~") + "/spark-master")
            subprocess.check_call(['ssh', address])
        else:
            print('\n')
            print >> sys.stderr, "No Spark job found, check status with qstat, or try a different jobid?"
            print('\n')


def destroy(jobId ):
    status = subprocess.check_output(["qstat", "-xml"])
    qtree = ET.fromstring(status)[0]
    jtree = ET.fromstring(status)[1]
    jobs = qtree.findall('job_list')
    if len(jobs) == 0:
        print('\n')
        print >> sys.stderr, "No running jobs found"
        print('\n')
    else:
        deleted = 0
        for job in jobs:
            jobclass = job.find('jclass_name').text
            state = job.find('state').text
            jobid = job.find('JB_job_number').text
            if (jobclass == version_short + ".default") and (state == 'r'):
                if jobId and jobid == jobId:
                    output = subprocess.check_output(['qdel', jobid])
                    deleted += 1
                else:
                    output = subprocess.check_output(['qdel', jobid])
                    deleted += 1
        if deleted == 0:
            print('\n')
            print >> sys.stderr, "No Spark jobs deleted, try a different jobid?"
            print('\n')
        else:
            print('\n')
            print('Spark jobs successfully deleted')
            print('\n')


def start():
    f = open(os.path.expanduser("~") + '/spark-master', 'r')
    master = f.readline().replace('\n','')

    if args.ipython is True:
       os.environ['IPYTHON_OPTS'] = "notebook --profile=nbserver"
       notebook = master[8:][:-5]
       print('\n')
       print('View your notebooks at https://' + notebook + ':9999')
       print('\n')

    os.environ['SPARK_HOME'] = version

    if os.getenv('PATH') is None:
        os.environ['PATH'] = ""

    if os.getenv('IPYTHON') is None:
       os.environ['IPYTHON'] = "1"

    os.environ['PATH'] = os.environ['PATH'] + ":" + "/usr/local/python-2.7.6/bin"
    os.environ['MASTER'] = master
    os.system(version + '/bin/pyspark')


def startScala():
    os.environ['SPARK_HOME'] = version

    if os.getenv('PATH') is None:
        os.environ['PATH'] = ""

    os.environ['PATH'] = os.environ['PATH'] + ":" + "/usr/local/python-2.7.6/bin"
    f = open(os.path.expanduser("~") + '/spark-master', 'r')
    os.environ['MASTER'] = f.readline().replace('\n','')
    os.system(version + '/bin/spark-shell')


def submit(master = ''):
    os.environ['SPARK_HOME'] = version

    if os.getenv('PATH') is None:
        os.environ['PATH'] = ""

    os.environ['PATH'] = os.environ['PATH'] + ":" + "/usr/local/python-2.7.6/bin"
    if master == '':
        with open(os.path.expanduser("~") + '/spark-master', 'r') as f:
            master = f.readline().replace('\n','')
    os.environ['MASTER'] = master
    os.system(version + '/bin/spark-submit --master ' + master + ' ' + args.submitargs)


def submitSSH(master = ''):
    os.environ['SPARK_HOME'] = version

    if os.getenv('PATH') is None:
        os.environ['PATH'] = ""

    os.environ['PATH'] = os.environ['PATH'] + ":" + "/usr/local/python-2.7.6/bin"
    if master == '':
        with open(os.path.expanduser("~") + '/spark-master', 'r') as f:
            master = f.readline().replace('\n','')
    os.environ['MASTER'] = master
    sparkCMD = version + '/bin/spark-submit --master ' + master + ' ' + args.submitargs
    sshCMD   = 'ssh'
    host     = master.lstrip('spark://').rstrip(':7077')
    subprocess.call( [ sshCMD, host, '%s' % sparkCMD ] )


def submitAndDestroy( master, jobId ):
    submitSSH(master)
    destroy(jobId)

if __name__ == "__main__":

    parser = argparse.ArgumentParser(description="launch and manage spark cluster jobs")
                        
    choices = ('launch', 'login', 'destroy', 'start', 'start-scala', 'submit', 'lsd')
                        
    parser.add_argument("task", choices=choices)
    parser.add_argument("-n", "--nnodes", type=int, default=2, required=False)
    parser.add_argument("-i", "--ipython", action="store_true")
    parser.add_argument("-v", "--version", choices=("1", "2", "3"), default="1", required=False)
    parser.add_argument("-j", "--jobid", type=int, default=None, required=False)
    parser.add_argument("-t", "--sleep_time", type=int, default=86400, required=False)
    parser.add_argument("-s", "--submitargs", type=str, default='', required=False)
                        
    args = parser.parse_args()
                        
    SPARKVERSIONS = {   
        '1': '/usr/local/spark-current',
        '2': '/usr/local/spark-master',
        '3': '/usr/local/spark-test'
    }                   
                        
    SPARKVERSIONSSHORT = {
        '1': 'spark',   
        '2': 'spark2',  
        '3': 'spark3'   
    }                   
                        
    version = SPARKVERSIONS[args.version]
    version_short = SPARKVERSIONSSHORT[args.version]
                        
    if args.nnodes == 1:
        raise Exception('Cannot start a Spark cluster with only 1 node, please request 2 or more.')
                        
    if args.task == 'launch':
        launch(str(args.sleep_time))
                        
    elif args.task == 'login':
        login(args.jobid or '')         
                        
    elif args.task == 'destroy':
        destroy(args.jobid or '')
                        
                        
    elif args.task == 'start':
        start()         
                        
                        
    elif args.task == 'start-scala':
        startScala()   
                        
                        
    elif args.task == 'submit':
        submit()        
                        
                        
    elif args.task == 'lsd':
        jobId  = launch(str(args.sleep_time))
        master = ''     
        while( master == '' ):
            master = findMasterByJobId(jobId)
            time.sleep(1) # wait 1 second to avoid spamming the cluster
        master = 'spark://%s:7077' % master
        print('\n')     
        print('%-20s%s\n%-20s%s' % ( 'job id:', jobId, 'spark master:', master ) )
        print('\n')
        p = multiprocessing.Process(target=submitAndDestroy, args=(master, jobId))
        p.start()       




